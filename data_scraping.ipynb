{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e150737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858df11a",
   "metadata": {},
   "source": [
    "# Data Extraction\n",
    "\n",
    "This notebook extracts, cleans, and stores data from the Rockfish website, documentation, news articles, research PDFs, and YouTube videos.  \n",
    "It outputs cleaned `.txt` files in the `Data` folder for downstream processing (like RAG-based chatbots).  \n",
    "\n",
    "**Key Features:**  \n",
    "- Scrapes and saves HTML pages  \n",
    "- Extracts PDF content while skipping math-heavy sections  \n",
    "- Retrieves YouTube auto-generated transcripts  \n",
    "- Organizes everything in `Data/`  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa7c80",
   "metadata": {},
   "source": [
    "## Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58b3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://www.rockfish.ai/\n",
      "Crawling: https://www.rockfish.ai/why-rockfish/about-us\n",
      "Crawling: https://www.rockfish.ai/why-rockfish/partners\n",
      "Crawling: https://www.rockfish.ai/platform/the-science\n",
      "Crawling: https://www.rockfish.ai/use-cases\n",
      "Crawling: https://www.rockfish.ai/news\n",
      "Crawling: https://www.rockfish.ai/contact-us\n",
      "Crawling: https://www.rockfish.ai/privacy-policy\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = 'https://www.rockfish.ai/'\n",
    "\n",
    "# Create the Data directory if it doesn't exist\n",
    "if not os.path.exists('Data'):\n",
    "    os.makedirs('Data')\n",
    "\n",
    "# To avoid re-visiting the same pages\n",
    "visited = set()\n",
    "\n",
    "def html_to_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove scripts and styles\n",
    "    for s in soup(['script', 'style']):\n",
    "        s.decompose()\n",
    "\n",
    "    # If the page has a <main> section, prioritize it\n",
    "    main = soup.find('main')\n",
    "    if main:\n",
    "        text = main.get_text(separator='\\n', strip=True)\n",
    "    else:\n",
    "        text = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "    return text\n",
    "\n",
    "def crawl_and_save(url, depth=0):\n",
    "    if url in visited or depth > 2:  # limit depth to avoid endless crawl\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(f'Crawling: {url}')\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f'Failed to fetch {url}: {e}')\n",
    "        return\n",
    "\n",
    "    visited.add(url)\n",
    "\n",
    "    # Clean the HTML to plain text\n",
    "    clean_text = html_to_text(response.text)\n",
    "\n",
    "    # Save the clean text to a .txt file\n",
    "    parsed_url = urlparse(url)\n",
    "    file_name = parsed_url.path.strip('/').replace('/', '_') or 'index'\n",
    "    file_path = os.path.join('Data', f'{file_name}.txt')\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    # Parse and find internal links\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for link_tag in soup.find_all('a', href=True):\n",
    "        href = link_tag['href']\n",
    "        next_url = urljoin(url, href)\n",
    "        # Only follow internal links\n",
    "        if urlparse(next_url).netloc == urlparse(BASE_URL).netloc:\n",
    "            crawl_and_save(next_url, depth + 1)\n",
    "\n",
    "    time.sleep(1)  # be nice!\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawl_and_save(BASE_URL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a71e9f",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43610a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://docs142.rockfish.ai/index.html\n",
      "Crawling: https://docs142.rockfish.ai/index.html#welcome-to-rockfish-data\n",
      "Crawling: https://docs142.rockfish.ai/index.html#get-started\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html#connect-to-the-rockfish-platform\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html#get-an-api-key\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html#set-up-the-rockfish-platform\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html#optional-rockfish-integration\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html#load-a-dataset\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html#train-a-synthetic-data-model\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html#generate-data\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html#evaluate-data-quality\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html#sql-queries\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html#whats-next\n",
      "Crawling: https://docs142.rockfish.ai/quick-start.html#need-assistance\n",
      "Crawling: https://docs142.rockfish.ai/installation.html\n",
      "Crawling: https://docs142.rockfish.ai/rockfish-cli.html\n",
      "Crawling: https://docs142.rockfish.ai/use-case-tutorials.html\n",
      "Crawling: https://docs142.rockfish.ai/use-case-integrate-rockfish.html\n",
      "Crawling: https://docs142.rockfish.ai/uc-demos/use-case-constraint-centralized-MLOps.html\n",
      "Crawling: https://docs142.rockfish.ai/connectors-overview.html\n",
      "Crawling: https://docs142.rockfish.ai/rf-overview.html\n",
      "Crawling: https://docs142.rockfish.ai/onboard-data.html\n",
      "Crawling: https://docs142.rockfish.ai/data-import.html\n",
      "Crawling: https://docs142.rockfish.ai/data-models.html\n",
      "Crawling: https://docs142.rockfish.ai/data-properties.html\n",
      "Crawling: https://docs142.rockfish.ai/recommendation.html\n",
      "Crawling: https://docs142.rockfish.ai/pre-processing.html\n",
      "Crawling: https://docs142.rockfish.ai/privacy.html\n",
      "Crawling: https://docs142.rockfish.ai/models.html\n",
      "Crawling: https://docs142.rockfish.ai/model-train.html\n",
      "Crawling: https://docs142.rockfish.ai/model-store.html\n",
      "Crawling: https://docs142.rockfish.ai/data-gen.html\n",
      "Crawling: https://docs142.rockfish.ai/storytelling.html\n",
      "Crawling: https://docs142.rockfish.ai/data-eval.html\n",
      "Crawling: https://docs142.rockfish.ai/evaluation-metrics.html\n",
      "Crawling: https://docs142.rockfish.ai/improve-quality.html\n",
      "Crawling: https://docs142.rockfish.ai/deploy-overview.html\n",
      "Crawling: https://docs142.rockfish.ai/deploy-types-overview.html\n",
      "Crawling: https://docs142.rockfish.ai/hybrid-deploy.html\n",
      "Crawling: https://docs142.rockfish.ai/enterprise-deploy.html\n",
      "Crawling: https://docs142.rockfish.ai/resources-overview.html\n",
      "Crawling: https://docs142.rockfish.ai/hardware.html\n",
      "Crawling: https://docs142.rockfish.ai/deploy-checklist.html\n",
      "Crawling: https://docs142.rockfish.ai/workers.html\n",
      "Crawling: https://docs142.rockfish.ai/docker-registry.html\n",
      "Crawling: https://docs142.rockfish.ai/deploy-docker-compose.html\n",
      "Crawling: https://docs142.rockfish.ai/operations-overview.html\n",
      "Crawling: https://docs142.rockfish.ai/troubleshooting.html\n",
      "Crawling: https://docs142.rockfish.ai/admin.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk-overview.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/rockfish.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/events.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/metrics.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/models.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/remote.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/streams.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/labs-recommender.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/labs-metrics.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/labs-steps.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/labs-vis.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/actions.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/actions-transformer.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/actions-tab-gan.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/actions-dg.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/actions-dataset.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/actions-models.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/actions-post-amplify.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/actions-apply-transform.html\n",
      "Crawling: https://docs142.rockfish.ai/sdk/actions-replace.html\n",
      "Crawling: https://docs142.rockfish.ai/api.html\n",
      "Crawling: https://docs142.rockfish.ai/support.html\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = 'https://docs142.rockfish.ai/index.html'\n",
    "\n",
    "# Create the Data directory if it doesn't exist\n",
    "if not os.path.exists('Data'):\n",
    "    os.makedirs('Data')\n",
    "\n",
    "# To avoid revisiting the same pages\n",
    "visited = set()\n",
    "\n",
    "# Common words to exclude (navigation, repeated on all pages)\n",
    "REPEATED_WORDS = set([\n",
    "    \"Rockfish Documentation\", \"Home\", \"Getting Started\", \"Quick Start\", \"Installation\",\n",
    "    \"SDK Installation\", \"CLI Installation\", \"Use Case Tutorials\", \"Summary\",\n",
    "    \"Generic Rockfish Integration\", \"Central Global Model Accuracy\", \"Integration\",\n",
    "    \"User Guide\", \"Overview\", \"Onboarding\", \"Import Data\", \"Data Models\",\n",
    "    \"Dataset Properties\", \"Recommendation Engine\", \"Pre-Processing\", \"Privacy\",\n",
    "    \"Training\", \"Models\", \"Train\", \"Model Store\", \"Generation\", \"Basic Generation\",\n",
    "    \"Storytelling\", \"Evaluation\", \"Metrics\", \"Improving Data Quality\", \"Deployment\",\n",
    "    \"Deployment Types\", \"Hybrid Deploy\", \"Table of contents\", \"Cuttlefish\",\n",
    "    \"Self hosted worker\", \"Deploy\", \"Enterprise\", \"Resources\", \"Hardware\",\n",
    "    \"Deployment Checklist\", \"Workers\", \"Docker\", \"Docker Registry\",\n",
    "    \"Docker Compose\", \"Operations\", \"Troubleshooting\", \"Administration\",\n",
    "    \"API\", \"Reference\", \"Welcome to Rockfish Data!\", \"Get started\",\n",
    "    \"rockfish\", \"rockfish.events\", \"rockfish.metrics\", \"rockfish.models\",\n",
    "    \"rockfish.remote\", \"rockfish.streams\", \"rockfish.labs.recommender\",\n",
    "    \"rockfish.labs.metrics\", \"rockfish.labs.steps\", \"rockfish.labs.vis\",\n",
    "    \"rockfish.actions\", \"rockfish.actions.transformer\", \"rockfish.actions.tab_gan\",\n",
    "    \"rockfish.actions.dg\", \"rockfish.actions.dataset\", \"rockfish.actions.models\",\n",
    "    \"rockfish.actions.amplify\", \"rockfish.actions.apply_transform\",\n",
    "    \"rockfish.actions.replace\", \"Developer\", \"Support\"\n",
    "])\n",
    "\n",
    "\n",
    "def html_to_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Get the <article> element with actual documentation content\n",
    "    article = soup.find('article', class_='md-content__inner')\n",
    "    if not article:\n",
    "        # fallback to entire page\n",
    "        article = soup\n",
    "\n",
    "    # Extract content respecting the document structure\n",
    "    lines = []\n",
    "    for el in article.descendants:\n",
    "        if el.name in ['h1', 'h2', 'h3', 'h4', 'h5']:\n",
    "            lines.append('\\n' + el.get_text(strip=True) + '\\n')\n",
    "        elif el.name == 'p':\n",
    "            text = el.get_text(separator=' ', strip=True)\n",
    "            if text:\n",
    "                lines.append(text)\n",
    "        elif el.name == 'pre':\n",
    "            code_text = el.get_text(separator=' ', strip=True)\n",
    "            lines.append('\\n' + code_text + '\\n')\n",
    "        elif el.name == 'li':\n",
    "            li_text = el.get_text(separator=' ', strip=True)\n",
    "            if li_text:\n",
    "                lines.append('- ' + li_text)\n",
    "\n",
    "    # Final cleanup: remove redundant empty lines\n",
    "    final_text = '\\n'.join(line for line in lines if line.strip())\n",
    "\n",
    "    return final_text\n",
    "\n",
    "\n",
    "\n",
    "def crawl_and_save(url, depth=0):\n",
    "    if url in visited or depth > 5:  # Adjust depth limit as needed\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(f'Crawling: {url}')\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f'Failed to fetch {url}: {e}')\n",
    "        return\n",
    "\n",
    "    visited.add(url)\n",
    "\n",
    "    # Clean text\n",
    "    clean_text = html_to_text(response.text)\n",
    "\n",
    "    # Generate file name based on URL path\n",
    "    parsed_url = urlparse(url)\n",
    "    page_name = parsed_url.path.strip('/').replace('/', '_') or 'index'\n",
    "    file_name = f'documentation-{page_name}.txt'\n",
    "    file_path = os.path.join('Data', file_name)\n",
    "\n",
    "    # Save clean text\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    # Parse and find internal links\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    for link_tag in soup.find_all('a', href=True):\n",
    "        href = link_tag['href']\n",
    "        next_url = urljoin(url, href)\n",
    "\n",
    "        # Only follow internal documentation links\n",
    "        if urlparse(next_url).netloc == urlparse(BASE_URL).netloc:\n",
    "            crawl_and_save(next_url, depth + 1)\n",
    "\n",
    "    time.sleep(1)  # Be polite\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawl_and_save(BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07532bfe",
   "metadata": {},
   "source": [
    "## Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f8d46ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: research-onprivacy.txt\n",
      "Saved: research-spectralnorm.txt\n",
      "Saved: research-practicalgan.txt\n",
      "Saved: research-raregan.txt\n",
      "Saved: research-imc20_doppelganger.txt\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Input and output folders\n",
    "RESEARCH_FOLDER = 'research'\n",
    "OUTPUT_FOLDER = 'Data'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "# Loop through each PDF in 'research' folder\n",
    "for pdf_file in os.listdir(RESEARCH_FOLDER):\n",
    "    if not pdf_file.lower().endswith('.pdf'):\n",
    "        continue\n",
    "\n",
    "    pdf_path = os.path.join(RESEARCH_FOLDER, pdf_file)\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    clean_text = ''\n",
    "    for page in doc:\n",
    "        # Extract text from the page\n",
    "        text = page.get_text()\n",
    "        lines = text.splitlines()\n",
    "\n",
    "        # Filter out likely math-heavy lines\n",
    "        for line in lines:\n",
    "            # Heuristic 1: skip lines with lots of math symbols\n",
    "            if any(sym in line for sym in ['=', '+', '-', '/', '*', '(', ')', '\\\\', '‚à´', 'Œ£', '‚àö', '‚àÜ', 'œÄ']):\n",
    "                continue\n",
    "            # Heuristic 2: skip very short lines (often math labels)\n",
    "            if len(line.strip()) < 10:\n",
    "                continue\n",
    "            # Keep the line\n",
    "            clean_text += line + '\\n'\n",
    "\n",
    "    # Save the cleaned text\n",
    "    doc_name = os.path.splitext(pdf_file)[0].replace(' ', '_')\n",
    "    output_filename = f'research-{doc_name}.txt'\n",
    "    output_path = os.path.join(OUTPUT_FOLDER, output_filename)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    print(f\"Saved: {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da977bc6",
   "metadata": {},
   "source": [
    "## News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6041801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Visiting: https://www.linkedin.com/posts/t-labs-telekom-laboratories_tchallenge-startups-activity-7290004012416811010-J9rX?utm_source=share&utm_medium=member_desktop\n",
      "Saved: news-t-labs-telekom-laboratories_tchallenge-startups-activity-7290004012416811010-J9rX.txt\n",
      "üîç Visiting: https://techcrunch.com/2025/01/15/rockfish-is-helping-enterprises-leverage-synthetic-data/\n",
      "Saved: news-rockfish-is-helping-enterprises-leverage-synthetic-data.txt\n",
      "üîç Visiting: https://www.businesswire.com/news/home/20250115547526/en/Rockfish-Data-Secures-Seed-Funding-to-Help-Enterprises-Leverage-Synthetic-Data-for-Operational-Workflows\n",
      "Timeout: Skipping https://www.businesswire.com/news/home/20250115547526/en/Rockfish-Data-Secures-Seed-Funding-to-Help-Enterprises-Leverage-Synthetic-Data-for-Operational-Workflows\n",
      "No clean text found for: https://www.businesswire.com/news/home/20250115547526/en/Rockfish-Data-Secures-Seed-Funding-to-Help-Enterprises-Leverage-Synthetic-Data-for-Operational-Workflows\n",
      "üîç Visiting: https://indicanews.com/the-rise-of-ai-generated-data/#google_vignette\n",
      "Saved: news-the-rise-of-ai-generated-data.txt\n",
      "üîç Visiting: https://www.axios.com/2024/07/27/synthetic-ai-data-effects?utm_campaign=editorial&utm_source=twitter&utm_medium=social\n",
      "Failed to fetch https://www.axios.com/2024/07/27/synthetic-ai-data-effects?utm_campaign=editorial&utm_source=twitter&utm_medium=social: 403 Client Error: Forbidden for url: https://www.axios.com/2024/07/27/synthetic-ai-data-effects?utm_campaign=editorial&utm_source=twitter&utm_medium=social\n",
      "No clean text found for: https://www.axios.com/2024/07/27/synthetic-ai-data-effects?utm_campaign=editorial&utm_source=twitter&utm_medium=social\n",
      "üîç Visiting: https://indicanews.com/2023/08/13/muckai-girish-column-synthetic-data-for-enterprises/\n",
      "Saved: news-muckai-girish-column-synthetic-data-for-enterprises.txt\n",
      "üîç Visiting: https://www.linkedin.com/posts/indica-news_muckai-girish-column-ask-not-what-you-can-activity-7108875938930597888-FZDn/\n",
      "Saved: news-indica-news_muckai-girish-column-ask-not-what-you-can-activity-7108875938930597888-FZDn.txt\n",
      "üîç Visiting: https://www.linkedin.com/posts/muckaigirish_generating-value-exploring-investment-opportunities-activity-7074136172662333440-WyMe?utm_source=share&utm_medium=member_desktop\n",
      "Saved: news-muckaigirish_generating-value-exploring-investment-opportunities-activity-7074136172662333440-WyMe.txt\n",
      "üîç Visiting: https://indicanews.com/2023/05/05/muckai-girish-column-generative-ai-for-enterprises/\n",
      "Saved: news-muckai-girish-column-generative-ai-for-enterprises.txt\n",
      "üîç Visiting: https://medium.com/@muckai/synthetic-data-for-enterprises-347f97bcce33\n",
      "Saved: news-synthetic-data-for-enterprises-347f97bcce33.txt\n",
      "üîç Visiting: https://medium.com/@muckai/synthetic-data-for-enterprises-347f97bcce33\n",
      "Saved: news-synthetic-data-for-enterprises-347f97bcce33.txt\n",
      "üîç Visiting: https://www.linkedin.com/posts/muckaigirish_ces2025-activity-7274964887435419649-c2vN?utm_source=share&utm_medium=member_desktop\n",
      "Saved: news-muckaigirish_ces2025-activity-7274964887435419649-c2vN.txt\n",
      "üîç Visiting: https://www.4yfn.com/news/4yfn-awards-finalists-revealed-ahead-of-mwc25-barcelona?utm_source=linkedin&utm_medium=social&utm_campaign=4yfn25_awardsblog\n",
      "Saved: news-4yfn-awards-finalists-revealed-ahead-of-mwc25-barcelona.txt\n",
      "üîç Visiting: https://www.dhs.gov/science-and-technology/news/2024/10/08/st-awards-contracts-four-startups-develop-privacy-enhancing-synthetic-data-generation-capabilities\n",
      "Saved: news-st-awards-contracts-four-startups-develop-privacy-enhancing-synthetic-data-generation-capabilities.txt\n",
      "üîç Visiting: https://www.youtube.com/watch?v=3gQKxDBumdY\n",
      "No clean text found for: https://www.youtube.com/watch?v=3gQKxDBumdY\n",
      "üîç Visiting: https://www.linkedin.com/posts/muckaigirish_as-the-saying-goes-it-takes-a-village-to-activity-7091118603252465664-902e?utm_source=share&utm_medium=member_desktop\n",
      "Saved: news-muckaigirish_as-the-saying-goes-it-takes-a-village-to-activity-7091118603252465664-902e.txt\n",
      "üîç Visiting: https://www.linkedin.com/feed/update/urn:li:activity:7089972257212510208\n",
      "Saved: news-urn:li:activity:7089972257212510208.txt\n",
      "üîç Visiting: https://www.tiecon.org/tie50\n",
      "Saved: news-tie50.txt\n",
      "üîç Visiting: https://www.linkedin.com/posts/muckaigirish_amazon-web-services-aws-activity-7085397067853934592-uver?utm_source=share&utm_medium=member_desktop\n",
      "Saved: news-muckaigirish_amazon-web-services-aws-activity-7085397067853934592-uver.txt\n",
      "üîç Visiting: https://www.armysbir.army.mil/news/award-6-million-ai-ml-technologies/\n",
      "Saved: news-award-6-million-ai-ml-technologies.txt\n",
      "üîç Visiting: https://www.einnews.com/pr_news/649860906/3nets-rockfish-data-enter-into-a-partnership-to-bring-generative-ai-and-multi-cloud-networking-solutions-to-customers\n",
      "Failed to fetch https://www.einnews.com/pr_news/649860906/3nets-rockfish-data-enter-into-a-partnership-to-bring-generative-ai-and-multi-cloud-networking-solutions-to-customers: 403 Client Error: Forbidden for url: https://www.einnews.com/pr_news/649860906/3nets-rockfish-data-enter-into-a-partnership-to-bring-generative-ai-and-multi-cloud-networking-solutions-to-customers\n",
      "No clean text found for: https://www.einnews.com/pr_news/649860906/3nets-rockfish-data-enter-into-a-partnership-to-bring-generative-ai-and-multi-cloud-networking-solutions-to-customers\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "# Load the raw HTML file you shared\n",
    "with open('news.html', 'r', encoding='utf-8') as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Create 'Data' folder\n",
    "if not os.path.exists('Data'):\n",
    "    os.makedirs('Data')\n",
    "\n",
    "# Find all article links\n",
    "article_links = soup.find_all('a', class_='col-grid-link-block w-inline-block')\n",
    "\n",
    "# Helper: extract main clean text from an external page\n",
    "def extract_clean_text(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)  # Timeout after 10 seconds\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout: Skipping {url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    main = soup.find('main') or soup.body\n",
    "    if not main:\n",
    "        return None\n",
    "\n",
    "    lines = []\n",
    "    for el in main.descendants:\n",
    "        if el.name in ['h1', 'h2', 'h3', 'h4', 'h5']:\n",
    "            lines.append('\\n' + el.get_text(strip=True) + '\\n')\n",
    "        elif el.name == 'p':\n",
    "            text = el.get_text(separator=' ', strip=True)\n",
    "            if text:\n",
    "                lines.append(text)\n",
    "        elif el.name == 'pre':\n",
    "            code_text = el.get_text(separator=' ', strip=True)\n",
    "            lines.append('\\n' + code_text + '\\n')\n",
    "        elif el.name == 'li':\n",
    "            li_text = el.get_text(separator=' ', strip=True)\n",
    "            if li_text:\n",
    "                lines.append('- ' + li_text)\n",
    "\n",
    "    cleaned_text = '\\n'.join(line for line in lines if line.strip())\n",
    "    return cleaned_text\n",
    "\n",
    "# Visit each link and save its content\n",
    "for link_tag in article_links:\n",
    "    href = link_tag.get('href')\n",
    "    if not href or not href.startswith('http'):\n",
    "        continue\n",
    "\n",
    "    # Get link text for a fallback filename\n",
    "    text_elem = link_tag.find('div', class_='research-caption')\n",
    "    if text_elem:\n",
    "        article_title = text_elem.get_text(separator=' ', strip=True)\n",
    "    else:\n",
    "        article_title = 'untitled-article'\n",
    "\n",
    "    parsed_url = urlparse(href)\n",
    "    slug = parsed_url.path.strip('/').split('/')[-1] or article_title.replace(' ', '_')[:30]\n",
    "    filename = f'news-{slug}.txt'\n",
    "    filepath = os.path.join('Data', filename)\n",
    "\n",
    "    # Visit and extract\n",
    "    print(f\"üîç Visiting: {href}\")\n",
    "    cleaned_text = extract_clean_text(href)\n",
    "    if cleaned_text:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_text)\n",
    "        print(f\"Saved: {filename}\")\n",
    "    else:\n",
    "        print(f\"No clean text found for: {href}\")\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae01027e",
   "metadata": {},
   "source": [
    "## Youtube Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "733c9eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtube_transcript_api\n",
      "  Downloading youtube_transcript_api-1.0.3-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting defusedxml<0.8.0,>=0.7.1 (from youtube_transcript_api)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/rf_rag/lib/python3.8/site-packages (from youtube_transcript_api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/rf_rag/lib/python3.8/site-packages (from requests->youtube_transcript_api) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/rf_rag/lib/python3.8/site-packages (from requests->youtube_transcript_api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/rf_rag/lib/python3.8/site-packages (from requests->youtube_transcript_api) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/rf_rag/lib/python3.8/site-packages (from requests->youtube_transcript_api) (2025.4.26)\n",
      "Downloading youtube_transcript_api-1.0.3-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: defusedxml, youtube_transcript_api\n",
      "Successfully installed defusedxml-0.7.1 youtube_transcript_api-1.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fbf90fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not get transcript for TrF1CU4Y2sc: no element found: line 1, column 0\n",
      "Saved: yt-5kT05Hv8QzE.txt\n",
      "üéâ Done!\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "OUTPUT_FOLDER = 'Data'\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "# List of YouTube video URLs\n",
    "video_urls = [\n",
    "    'https://www.youtube.com/watch?v=TrF1CU4Y2sc&t=43s',\n",
    "    'https://www.youtube.com/watch?v=5kT05Hv8QzE&t=1814s'\n",
    "]\n",
    "\n",
    "for url in video_urls:\n",
    "    # Extract video ID\n",
    "    if 'v=' in url:\n",
    "        video_id = url.split('v=')[1].split('&')[0]\n",
    "    else:\n",
    "        print(f\"Could not extract video ID for {url}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Get the transcript\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "        # Merge all text into one plain text\n",
    "        transcript_text = '\\n'.join([entry['text'] for entry in transcript])\n",
    "\n",
    "        # Save it to a file\n",
    "        output_filename = f'yt-{video_id}.txt'\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, output_filename)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(transcript_text)\n",
    "\n",
    "        print(f\"Saved: {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get transcript for {video_id}: {e}\")\n",
    "\n",
    "print(\"üéâ Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd9d5d",
   "metadata": {},
   "source": [
    "#### Sources\n",
    "- Research Papers\n",
    "- New articles\n",
    "- Web Pages\n",
    "- Documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rf_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
