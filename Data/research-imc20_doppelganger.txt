Using GANs for Sharing Networked Time Series Data:
Challenges, Initial Promise, and Open Questions
Carnegie Mellon University
Pittsburgh, PA
zinanl@andrew.cmu.edu
Alankar Jain
Carnegie Mellon University
Pittsburgh, PA
alankarjain91@gmail.com
New York, NY
Chen.Wang1@ibm.com
Giulia Fanti
Carnegie Mellon University
Pittsburgh, PA
gfanti@andrew.cmu.edu
Vyas Sekar
Carnegie Mellon University
Pittsburgh, PA
vsekar@andrew.cmu.edu
search and development in the networked systems community.
a generic framework for sharing synthetic datasets with minimal
expert knowledge. As a specific target, our focus in this paper is
of existing GAN approaches for such workloads with respect to
models. Although we do not resolve the privacy problem in this
erties of GANs, and suggest a potential roadmap for addressing
these challenges. By shedding light on the promise and challenges,
we hope our work can rekindle the conversation on workflows for
data sharing.
CCS CONCEPTS
ologies →Knowledge representation and reasoning.
works, privacy
 IMC ’20, October 27–29, 2020, Virtual Event, USA 
ACM Reference Format:
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. 2020.
Using GANs for Sharing Networked Time Series Data: Challenges, Initial
Promise, and Open Questions. In ACM Internet Measurement Conference
INTRODUCTION
allows network operators and system designers to explore design
driven research are restricted to those who possess data. Even when
they are reluctant to share datasets for fear of revealing business
in the networking and systems communities.
One alternative is for data holders to create and share synthetic
cesses in our community where experts identify the key factors
of specific traces that impact downstream applications and create
generative models using statistical toolkits [5, 29, 31, 43, 64, 68, 78–
81, 97, 98, 109, 110, 115]. Unfortunately, this approach requires
significant human expertise and does not easily generalize across
workloads and use cases.
workload characteristics and downstream tasks? Such a toolkit
it easier to obtain and share data.
This paper explores if and how we can leverage recent advances
fit GANs offer is the ability to learn high fidelity representations
by the excitement in generating photorealistic images [65], among
other applications. A secondary benefit is that GANs allow users to
This work is licensed under a Creative Commons Attribution International 4.0 License
IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
To scope our work, we consider an important and broad class of
measurements [15, 54, 90].
We identify key disconnects between existing GAN approaches
and our use case on two fronts:
• With respect to fidelity, we observe key challenges for existing
correlations within time series, such as diurnal patterns, which
more, on datasets with a highly variable dynamic range GANs
exhibit severe mode collapse [6, 50, 70, 101], wherein the GAN
generated data only covers a few classes of data samples and
ignores other modes of the distribution.
portant as practitioners are often worried that GANs may be
“memorizing” the data and inadvertently reveal proprietary
information or suffer from deanonymization attacks [8, 67,
training techniques may sacrifice the utility of the data [3, 14,
35, 40, 112, 113], and it is not clear if they apply in our context.
Our primary contribution is the design of a practical workflow
sights and concurrent advances in the GAN literature to tackle the
the generation of metadata from time series and feeds metadata to
the time series generator at each time step, and also introduces an
auxiliary discriminator for the metadata generation. This contrasts
with conventional approaches where these are generated jointly.
Second, to tackle mode collapse, our GAN architecture separately
ries, which can then be rescaled back to the realistic range. Third, to
capture temporal correlations, DG outputs batched samples rather
eling [44], its use in GANs is relatively preliminary [70, 92] and
structural microbenchmarks of each dataset better than baseline
predictors trained on DG generated data have test accuracies up to
offs of GANs, which is an open challenge in the ML community
as well [62]. Resolving these tradeoffs is beyond the scope of this
work. However, we empirically confirm that an important class of
ing DG on larger datasets. This may run counter to conventional
release practices, which advocate releasing smaller datasets to avoid
leaking user data [91]. A second positive result is that we highlight
that the decoupled generation architecture of DG workflow can
we empirically evaluate recent proposals for GAN training with
differential privacy guarantees [3, 14, 35, 40, 112, 113] and show
that these methods destroy temporal correlations even for moderate
privacy guarantees, highlighting the need for further research on
the privacy front.
Roadmap: In the rest of the paper, we begin by discussing use
cases and prior work in §2.2. We provide background on GANs and
challenges in §3. We describe the design of DG in §4 and evaluate it
in §5. We analyze privacy tradeoffs in §6, before concluding in §7.
MOTIVATION AND RELATED WORK
In this section, we discuss motivating scenarios and why existing
solutions fail to achieve our goals.
Use cases and Requirements
While there are many scenarios for data sharing, we consider two
a network operator collaborating with an equipment vendor to
design custom workload optimizations. Enterprises often impose
open research: Many research proposals rely on datasets to test and
develop ideas. However, policies and business considerations may
preclude datasets from being shared, thus stymieing reproducibility.
In such scenarios, we consider three representative tasks:
to understand the shortcomings in existing systems and explore
remedial solutions [11, 16, 60, 104]. In this case, generated data
should preserve trends and distributions well enough to reveal such
structural insights.
models, especially for tasks like resource allocation [42, 61, 69].
For these models to be useful, they should have enough fidelity
that a predictor trained on generated data should make meaningful
predictions on real data.
rameters. A key property for generated data is that if algorithm A
performs better than algorithm B on the real data, the same should
hold on the generated data.
ples include: 1. Web traffic traces of webpage views with metadata
lations [102], or generate recommendations [37, 86]; 2. Network
measurements of packet loss rate, bandwidth, delay with metadata
agement [61]; or 3. Cluster usage measurements of metrics such as
[75]. At a high level, each example consists of time series samples
and associated metadata that can be either numeric or categorical
Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
teractions between agents; e.g., generating full TCP session packet
Across these use cases and datasets, we require techniques that
given a website page view dataset, website category prediction
focuses on the dependency between the number of page views and
its category, whereas page view modeling only needs the temporal
els for each use case and dataset is time consuming and requires
significant human expertise. Ideally, we need generative techniques
that are general across diverse datasets and use cases and achieve
high fidelity.
Related work and limitations
working domain falls in two categories: simulation models and
Simulation models: These generate data by building a simulator
that mimics a real system or network [30, 59, 73, 84, 95, 98, 99].
GloudSim [30] is a distributed cloud simulator for generating cloud
workload and traces. In terms of fidelity, this class of models is good
if the simulator is very close to real systems. However, in reality,
it is often hard to configure the parameters to simulate a given
parameters have been proposed [30, 73, 84, 95], it is still difficult to
ensure that the simulator itself is close to the real system. Moreover,
they do not generalize across datasets and use cases, because a new
simulator is needed for each scenario.
a mathematical model instead of using a simulation. Specifically,
domain expects determine which parameters are important and
rameters can be manually configured [1, 27, 107] or learned from
lenging to come up with models and parameters that achieve high
fidelity. For example, BURSE [115] explicitly models the burstiness
times, but does not model the correlation between job metadata
different datasets and use cases require different models.
ples include autoregressive models, Markov models, and recurrent
Autocorrelation
DoppelGANger
DoppelGANger
MarketSimulator
Figure 1: Autocorrelation of daily page views for Wikipedia
Web Traffic dataset.
have the potential to generalize across datasets. However, they are
not general in terms of use cases, because they do not jointly model
metadata and time series. For example, in a network measurement
ing its patterns. But if we want to learn which IP prefix has network
term temporal correlations. For instance, all these models fail to
of learning autocorrelations,1 but to show that learning temporal
annual correlations without special tuning.
As we saw, the above classes of techniques do not achieve good
ing goal is thus to develop a general framework that can achieve
high fidelity with minimal expertise.
Problem formulation
O1,O2, ...,On	
2, ...,Aim].
For example, metadata Ai
tion, and Ai
2 the client’s ISP. Note that we can support datasets
in which multiple samples have the same set of metadata. The
T i ], where Ri
ments. The number of records for sample Oi is given by T i. Each
j , and K measurements
j,2, ..., f i
j,K]. For example, ti
j represents the time when
1While there are specific tools for estimating autocorrelation, in general this is a hard
problem in high dimensions [10, 18].
IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
Figure 2: Original GAN architecture from [46].
the measurement f i
j is taken, and f i
j,2 represent the ping loss
rate and traffic byte counter at this timestamp respectively. Note
that the timestamps are sorted, i.e. ti
ing applications. For example, it is able to express web traffic and
as input and learn a model that can generate a new dataset D′ as
output. D′ should exhibit fidelity, and the methodology should be
general enough to handle datasets in our abstraction.
GANs: Background and Promise
take as input training data samples and output a model that can
produce new samples from the same distribution as the original data.
More precisely, if we have a dataset of n samples O1, . . . ,On, where
Oi ∈Rp, and each sample is drawn i.i.d. from some distribution
Oi ∼PO. The goal of GANs is to use these samples to learn a model
that can draw samples from distribution PO [46].
tiated with neural networks. In the canonical GAN design [46], the
generator maps a noise vector z ∈Rd to a sample O ∈Rp, where
a Gaussian or a uniform. Simultaneously, we train the discriminator
fication task are used to train the parameters of both the generator
and discriminator through backpropagation. The loss function for
ially. Unlike prior generative modeling approaches which likelihood
assumptions about the data structure.
efits. First, similar to the machine learning models, GANs can be
general across datasets. The discriminator is an universal agent for
judging the fidelity of generated samples. Thus, the discriminator
only needs raw samples and it does not need any other information
about the system producing the samples. Second, GANs can be used
GANs have been used in other domains for generating realistic
[36, 118], and music [32, 82].
Using GANs to Generate Time Series
Prior Work. Using GANs to generate time series is a popular
time [35, 42, 117–119]. These works typically change two aspects
of the GAN: the architecture [35, 42, 119], the training [118], or
both [17, 117]. The two most relevant papers to ours are RCGAN
[35] and TimeGAN [117]. RCGAN is the most similar design to
time series and can condition the generation on metadata. However,
uation of the correlations across time series and between metadata
and measurements. We found its fidelity on our datasets to be poor;
we instead use a different discriminator architecture, loss function,
RNNs for both the generator and discriminator. Unlike RCGAN, it
trains an additional neural network that maps time series to vector
embeddings, and the generator outputs sequences of embeddings
ded time series is common, both in approaches that rely on GANs
Challenges. Next, we highlight key challenges that arise in
using GANs for our use cases. While these challenges specifically
inspired use cases, some of these challenges broadly apply to other
use cases as well.
poral correlations. As we see in Figure 1, the canonical GAN does
poorly in capturing temporal correlations trained on the Wikipedia
GANs for other time series data has also observed this [35, 36, 117,
118]. One approach to address this is segmenting long datasets
into chunks; e.g., TimeGAN [117] chunks datasets into smaller
time series each of 24 epochs, and only evaluates the model on
producing new time series of this length [116]. This is not viable
GANs where they generate only a few modes of the underlying
distribution [6, 50, 70, 101]. It is particularly exacerbated in our
time series use cases because of the high variability in the range
for such tasks is unknown [35, 117, 119], and directly generating
dimension will break their correlations.
with prior work [24, 39, 49, 52].
Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
rolled representation conveys that the RNN is being used
many times to generate samples.
Privacy Challenges: In addition to the above fidelity challenges,
question, not unique to our work, is finding the right definition of
privacy for each use case. Some commonly used definitions in the
these definitions can hurt fidelity [9, 93] without defending against
may also want to even hide specific features and avoid releasinng
arise: First, can GANs support these flexible notions of privacy in
practice, and if so under what configurations? Second, there are
sufficient to be practical for networking datasets?
DOPPELGANGER DESIGN
In this section, we describe how we tackle fidelity shortcomings
of time series GANs. Privacy is discussed in Section 6. Recall that
existing approaches have issues in capturing temporal effects and
relations between metadata and measurements. In what follows,
we present our solution starting from the canonical GAN strawman
and extend it to address these challenges. Finally, we summarize
the design and guidelines for users to use our workflow.
RNN primer and limitations: Similar to prior efforts, we posit
that the main reason is that MLPs are not well suited for time series.
are designed to model time series and have been widely used in the
GAN literature to generate time series [35, 82, 117–119]. Specifically,
generating the entire time series at once, RNNs generate one record
Mean square error
Figure 4: Error vs. batch parameter.
key difference in a RNN from traditional neural units is that RNNs
have an internal state that implicitly encodes all past states of the
signal. Thus, when generating Ri
j, the RNN unit can incorporate
the patterns in Ri
Note that RNNs can learn correlations across the dimensions of a
However, we empirically find that RNN generators still struggle
to capture temporal correlations when the length exceeds a few
hundred epochs. The reason is that for long time series, RNNs take
too many passes to generate the entire sample; the more passes
taken, the more temporal correlation RNNs tend to forget. Prior
work copes with this problem in three ways. The first is to generate
only short sequences [82, 117, 118]; long datasets are evaluated on
chunks of tens of samples [116, 117]. The second approach is to
train on small datasets, where rudimentary designs may be able
[28] generates time series of length 1,000, from a dataset of about
defeats the purpose of training a model. A third approach assumes
an auxiliary raw data time series as an additional input during the
generation phase to help generate long time series [119]. This again
defeats the purpose of synthetic data generation.
Our approach: To reduce the number of RNN passes, we propose
to use a simple yet effective idea called batch generation. At each
effectively reduces the total number of RNN passes by a factor of S.
As S gets larger, the difficulty of synthesizing a batch of records at
between the number of RNN passes and the single pass difficulty.
For example, Figure 4 shows the mean square error between the
autocorrelation of our generated signals and real data on the WWT
for many datasets and a simple autotuning of this hyperparameter
nores the timestamps from generation. In practice, for some datasets,
standard practice of computing gradients on small sets of samples rather than the full
dataset for efficiency [56]. Generating batches of sequences in SeqGAN [118] involves
generating multiple time series during GAN training to estimate the reward of a
generator policy in their reinforcement learning framework. Both are orthogonal to
our batch generation.
IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
Normalized
page views
Normalized
page views
show telltale signs of mode collapse as they have similar
shapes and amplitudes.
may be important for downstream systems and networking tasks.
To this end, we support two simple alternatives. First, if the raw
timestamps are not important, we can assume that they are equally
erties are critical, we can simply use the initial timestamp of each
measurement.
Tackling Mode Collapse
puts homogeneous samples despite being trained on a diverse
dataset. For example, suppose we train on web traffic data that
includes three distinct kinds of signals, corresponding to different
only one of those traffic types.
trained on the WWT dataset, normalized and shifted to [−1, 1]. The
amplitudes, offsets, and shapes.4
Existing work and limitations: Alleviating mode collapse is an
mitigating mode collapse [50, 70]. However, these did not resolve
the problem on our datasets.
Our approach: Our intuition is that unlike images or medical data,
where value ranges tend to be similar across samples, networking
datasets exhibit much higher range variability. Datasets with a large
they have a more diverse set of modes, making them harder to learn.
For example, in the WWT dataset, some web pages consistently
have >2000 page views per day, whereas others always have <10.
4While mode collapse can happen both in measurements or in metadata, we observed
substantially more mode collapse in the measurements.
Rather than using a general solution for mode collapse, we build
simply normalize this data by the global min and max, store them as
global constants, and train on the normalized data. However, this is
just scaling and shifting by a constant; from the GAN’s perspective,
the learning problem is the same, so mode collapse still occurs.
Instead, we normalize each time series signal individually, and
individually, which are then used to rescale measurements to a
realistic range.
Note that this approach differs from typical feature normalization
and minimum value of each time series as a random variable to
same range during generation, which alleviates the mode collapse
normalization on the WWT data, we generate samples with a broad
range of amplitudes, offsets, and shapes.
Capturing attribute relationships
surements. For example, fiber users tend to use more traffic than
bution between measurements and metadata. As discussed in §3.3.2,
naively generating concatenated metadata Ai and measurements Ri
does not learn the correlations between them well. We hypothesize
that this is because jointly generating metadata and measurements
using a single generator is too difficult.
Existing work and limitations: A few papers have tackled this
problem, mostly in the context of generating multidimensional
ple, prior works [35, 41, 119] learn a conditional model in which
ates measurements conditioned on the attributes; generating the
attributes as well is a simple extension [35]. TimeGAN claims to
on any datasets that include metadata in the paper, nor does the
released code handle metadata [116, 117].
min is always less than our max.
Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
Our Approach: We start by decoupling this problem into two
based architecture from §4.1. To preserve the hidden relationships
data Ai is added as an input to the RNN at every step.
Recall from section 4.2 that we treat the max and min of each
generated real and fake metadata as inputs, generate measurements
Unfortunately, a decoupled architecture alone does not solve
the problem. Empirically, we find that when the average length of
data—especially the metadata—is poor. To understand why, recall
that a GAN discriminator judges the fidelity of generated samples
and provides feedback for the generator to improve. When the total
sample fidelity is hard.
Motivated by this, we introduce an auxiliary discriminator which
discriminates only on metadata. The losses from two discriminators
tor effectively learns from this auxiliary discriminator to generate
criminator, the generator can learn to generate measurements well.
metadata distribution from DG on the WWT dataset. That is, for
each time series, we extract the maximum and minimum value, and
ages over many time series. This distribution implicitly reflects how
well DG reproduces the range of time series values in the dataset.
We observe that adding the auxiliary discriminator significantly
improves the fidelity of the generated distribution, particularly in
the tails of the true distribution.
Putting it all together
The overall DG architecture is in Figure 7, highlighting the key
pled generation of metadata and measurements using an auxiliary
discriminator, and conditioning the measurements based on the
metadata generated. Second, to address the mode collapse problem
for the measurements, we add the fake metadata capturing the
DoppelGANger
DoppelGANger
Discriminator
Discriminator
conditioned generation to 
Normalization to tackle 
RNN with batched generation 
Auxiliary discriminator 
to improve fidelity 
Discriminator
tensions to canonical GAN approaches.
RNN generator to capture the temporal correlations and synthesize
long time series that are representative.
The training phase requires two primary inputs: the data schema
tuning that data holders sharing a dataset using DG need to be
trols the number of measurements generated at each RNN pass.
IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
Correlated
dimensional
measurements
Table 1: Challenging properties of studied datasets.
specify sensitive metadata, whose distribution can be masked or
holders sharing the generative model with the data users. Users can
then flexibly use this model and also optionally specify different
That said, our workflow also accommodates a more restrictive mode
of sharing, where the holder uses DG to generate synthetic data
internally and then releases the generated data without sharing the
FIDELITY EVALUATION
We evaluate the fidelity of DG on three datasets, whose properties
are summarized in Table 17.
measurement lengths.
of daily views of Wikipedia articles, starting from July 1st, 2015
to December 31st, 2016 [47]. In our language, each sample is a
page view counter for one Wikipedia page, with three metadata:
of daily page views.
reflecting client’s aggregate Internet usage.
tains usage traces of a Google Cluster of 12.5k machines over 29
days in May 2011. We use the logs containing measurements of
task resource usage, and the exit code of each task. Once the task
6From a privacy perspective, model and data sharing may suffer similar information
leakage risks [53], but this may be a pragmatic choice some providers can make
nonetheless.
7Our choice of using public datasets is to enable others to independently validate and
reproduce our work.
which we treat as an metadata.
Appendix A.
Baselines. We only compare DG to the baselines in §2.2
reproducibility, we provide complete configuration details for these
used for generating time series data, there is no natural way to
jointly generate metadata and time series in HMMs. Hence, we
infer a separate multinomial distribution for the metadata. During
generation, metadata are randomly drawn from the multinomial
distribution on training data, independently of the time series.
can only learn to generate measurements. In order to jointly learn
to generate metadata and measurements, we design the following
is randomly drawn from the multinomial distribution on training
data, and the first record R1 is drawn a Gaussian distribution learned
from training data.
train an RNN via teacher forcing [111] by feeding in the true time
series at every time step and predicting the value of the time series
erate the time series by using its predicted output as the input for
the next time step. A traditional RNN can only learn to generate
measurements. We design an extended RNN takes metadata A as
an additional input. During generation, A is randomly drawn from
the multinomial distribution on training data, and the first record
R1 is drawn a Gaussian distribution learned from training data.
ries of different lengths, so several of our evaluations cannot be run
and compared against it.
RCGAN [35]: RCGAN does not generate metadata, and only deals
tions cannot be run on RCGAN. To make a comparison, we used the
approach [17] designed to generate synthetic financial market data,
since its code is publicly available.
Metrics. Evaluating GAN fidelity is notoriously difficult [72,
image data [55, 92] and cannot be applied to our datasets. Moreover,
Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
Pearson correlation
DoppelGANger
DoppelGANger
Figure 8: CDF of Pearson correlation between CPU rate and
assigned memory usage from GCUT.
numeric metrics do not always capture the qualitative problems of
generative models. We therefore evaluate DG with a combination
of qualitative and quantitative microbenchmarks and downstream
tasks that are tailored to each of our datasets. Our microbenchmarks
evaluate how closely a statistic of the generated data matches the
tocorrelations, and the similarity can be evaluated qualitatively or
tion or algorithm comparison. In line with the recommendations of
metrics like prediction accuracy. Each metric is explained in more
detail inline.
tions [79], we explore how DG captures structural data properties
measurement joint distributions.8
lations, Figure 1 shows the average autocorrelation for the WWT
by the periodic weekly spikes and the local peak at roughly the
lower mean square error from the true data autocorrelation than
ticularly since we are using an RNN generator. Typically, RNNs
are able to reliably generate time series of length around 20, while
the length of WWT measurements is 550. We believe this is due
feature hurts the learned autocorrelation. TimeGAN and RCGAN,
for instance, use RNNs and adversarial training but does not batch
8 Such properties are sometimes ignored in the ML literature in favor of downstream
marks are important.
DoppelGANger
Figure 9: Histogram of task duration for the Google Cluster
but DoppelGANger captures it.
tural differences [35, 117]. E.g., WWT is an order of magnitude
longer than the time series it evaluates on [34, 116].
Another aspect of learning temporal correlations is generating
time series of the right length. Figure 9 shows the duration of tasks
in the GCUT dataset for real and synthetic datasets generated by
DG and RNN. Note that TimeGAN generates time series of different
lengths by first generating time series of a maximum length and
then truncating according to the empirical length distribution from
the training data [116]. Hence we do not compare against TimeGAN
because the comparison is not meaningful; it perfectly reproduces
the empirical length distribution, but not because the generator is
learning to reproduce time series lengths.
DG’s length distribution fits the real data well, capturing the
bimodal pattern in real data, whereas RNN fails. Other baselines
We observe this regularly; while DG captures multiple data modes,
our baselines tend to capture one at best. This may be due to the
naive randomness in the other baselines. RNNs and AR models
fied duration distributions; HMMs instead are too random: they
maintain too little state to generate meaningful results.
relation between the CPU and memory measurements of generated
samples from the GCUT dataset. Figure 8 shows the CDF of these
correlation coefficients for different time series. We observe that
distribution than any of our baselines.
Measurement distribution: As discussed in §4.3 and Figure 7, DG
dataset. As a comparison, TimeGAN and RCGAN have much worse
fidelity. TimeGAN captures the two modes in the distribution, but
fails to capture the tails. RCGAN does not learn the distribution
at all. In fact, we find that RCGAN has severe mode collapse in
in RCGAN is 30 [34], whereas the sequence length in WWT is 550,
generator and discriminator updates per step in different datasets
IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
DoppelGANger
End Event Type
Figure 10: Histograms of end event types from GCUT.
DoppelGANger
RNN HMM Naive GAN
tion of DSL and cable users. Lower is better.
out tuning the numbers of generator and discriminator updates.
Metadata distribution: Learning correct metadata distributions is
tioned in §5.1.2, for our HMM, AR, and RNN baselines, metadata
are randomly drawn from the multinomial distribution on training
data because there is no clear way to jointly generate metadata
and measurements. Hence, they trivially learn a perfect metadata
distribution. Figure 10 shows that DG is also able to mimic the
real distribution of end event type distribution in GCUT dataset,
while naive GANs miss a category entirely; this appears to be due
to mode collapse, which we mitigate with our second discriminator.
Results on other datasets are in Appendix C.
measurement distribution. To illustrate this, we compute the CDF
of total bandwidth for DSL and cable users in MBA dataset. Table 2
the ground truth,9 showing that DG is closest to the real distribution.
CDF figures are attached in Appendix C.
lent to memorizing the training data, which is a common concern
with GANs [7, 88]. To evaluate this, we ran an experiment inspired
by the methodology of [7]: for a given generated DG sample, we
the generated samples and the nearest neighbors on all datasets,
suggesting that DG is not memorizing. Examples can be found in
Appendix C.
#Training samples
Mean square error
Figure 11: Mean square error of autocorrelation of the
daily page views v.s. number of training samples for WWT
dataset. For each training set size, 5 independent runs are
executed and their MSE are plotted in the figure. The line
connects the median MSE of the 5 independent runs.
MarketSimulator
DoppelGANger
sented. Except the last row, all models are trained with 50000
training samples.
Resource costs: DG has two main costs: training data and training
between the generated samples’ autocorrelations and the real data’s
autocorrelations on the WWT dataset as a function of training set
size. MSE is sensitive to training set size—it decreases by 60% as
the training data grows by 2 orders of magnitude. However, Table 3
samples in autocorrelation MSE. Figure 11 also illustrates variability
between models; due to GAN training instability, different GAN
models with the same hyperaparameters can have different fidelity
metrics. Such training failures can typically be detected early in
the training proccess.
With regards to training time, Table 4 lists the training time
for DG and other baselines. All models were trained on a single
NVIDIA Tesla V100 GPU with 16GB GPU memory and an Intel
Xeon Gold 6148 CPU with 24GB RAM. These implementations have
not been optimized for performance at all, but we find that on the
WWT dataset, DG requires 17 hours on average to train, which is
Predictive modeling: Given time series measurements, users may
want to predict whether an event E occurs in the future, or even
Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
Market Simulator
DoppelGANger
els on the WWT dataset. All models are trained with 50000
training samples.
Generated Data
Generative 
Figure 12: Predictive modeling setup: Using training data A,
we generate samples B ∪B′. Subsequent experiments train
downstream tasks on A or B, our training sets, and then test
on A′ or B′.
forecast the time series itself. For example, in GCUT dataset, we
could predict whether a particular job will complete successfully.
In this use case, we want to show that models trained on generated
data generalize to real data.
We first partition our dataset, as shown in Figure 12. We split real
data into two sets of equal size: a training set A and a test set A’. We
A. We generate datasets B and B’ for training and testing. Finally,
we evaluate event prediction algorithms by training a predictor on
the generalization abilities of the prediction algorithms both within
be useful for cluster resource allocators. This prediction task reflects
the correlation between the time series and underlying metadata
gorithms to demonstrate the generality of our results: multilayer
tor when trained on generated data and tested on real. Real data
expectedly has the highest test accuracy. However, we find that
DG performs better than other baselines for all five classifiers. For
ings are preserved on generated data on the GCUT dataset by
NaiveBayes
LogisticRegression
DecisionTree
DoppelGANger
DoppelGANger
RNN HMM Naive GAN
Table 5: Rank correlation of predication algorithms on
GCUT and WWT dataset. Higher is better.
evaluate this on the WWT dataset by training different regression
algorithms on real data to be preserved when we train and test
them on generated data. In other words, for each class of generated
data, we train each of the predictive models on B and test on B′.
This is different from Figure 13, where we trained on generated
with the ground truth ranking, in which the predictive models are
trained on A and tested on A′. We then compute the Spearman’s
ing in generated data is correlated with the groundtruth ranking.
Table 5 shows that DG and AR achieve the best rank correlations.
domness, so all predictors achieve the same high accuracy; the AR
rank correlation together with other fidelity metrics. More results
Other case studies
DG is being evaluated by several independent users, though DG
has not yet been used to release any datasets to the best of our
idated the fidelity of DG. IBM stores time series data of resource
usage measurements for different containers used in the cloud’s
machine learning service. They trained DG to generate resource
usage measurements, with the container image name as metadata.
Figure 14 shows the learned distribution of containers’ maximum
usually size containers according to their peak usage. DG captures
such as banking, sensor data, and natural resource modeling have
also been using DG [26, 96].
IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
Figure 14: Maximum CPU usage.
PRIVACY ANALYSIS AND TRADEOFFS
this section, we illustrate what GANs can and cannot do in each of
these topics.
Protecting business secrets
In our discussions with major data holders, a primary concern about
data sharing is leaking information about the types of resources
available and in use at the enterprise. Many such business secrets
tant for downstream applications, so data holders cannot simply
discard hardware type from data.
tribution. The naive way is to rejection sample the metadata to a
different desired distribution. This approach is clearly inefficient.
The architecture in §4.3 naturally allows data holders to obfuscate
the metadata distribution in a much simpler way. After training on
the original dataset, the data holders can retrain only the metadata
generator to any desired distribution as the metadata generator and
measurement generator are isolated. Doing so requires synthetic
metadata of the desired distribution, but it does not require new
time series data.
A major open question is how to realistically tune attribute
distributions. Both of the above approaches to obfuscating attribute
a reasonable approximation for small perturbations, changing the
For example, Ai may represent the fraction of large jobs in a system,
and Ri the memory usage over time; if we increase Ai, at some point
we should encounter resource exhaustion. However, since the GAN
is trained only on input data, it cannot predict such effects. Learning
to train GANs with simulators that can model physical constraints
of systems may be a good way to combine the statistical learning
properties of GANs with systems that encode domain knowledge.
Protecting user privacy
User privacy is a major concern with regards to any data sharing
application, and generative models pose unique challenges. For
#Training samples
Attack success rate
Figure 15: Membership inference attack against DG in WWT
dataset vs. training set size.
vacy. One challenge is the difficulty of defining what it means to be
definitions [33, 93, 106], a common theme is deniability: released
data or models should look similar whether a given user’s data is
included or not.
In this section, we show how two of the most common notions
of deniability relate to GANs.
Differential privacy: A leading metric for measuring user privacy
learning, DP states that the trained model should not depend too
much on any individual user’s data. More precisely, a model M
and D′ that differ in the record of a single user, and for any input
z, it holds that
Smaller values of ϵ and δ give more privacy.
Recent work has explored how to make deep learning models
ent updates in stochastic gradient descent to ensure that any single
ence the model parameters. Researchers have applied this technique
to generating time series.
sorFlow Privacy [4].10 Figure 16 shows the autocorrelation of the
resulting time series for different values of the privacy budget, ϵ.
to destroy our autocorrelation plots, this was not always evident
functions that are not supported by Tensorflow Privacy [116].
Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
from downstream metrics, such as predictive accuracy in [35]. This
tatively and quantitatively; prior work has focused mainly on the
generation.
Membership Inference: Another common way of evaluating
user deniability is through membership inference attacks [22, 53, 94].
Given a trained machine learning model and set of data samples,
the goal of such an attack is to infer whether those samples were in
the training dataset. The attacker does this by training a classifier to
ferential privacy should protect against such an attack; the stronger
the privacy parameter, the lower the success rate. However, DP
alone does quantify the efficacy of membership inference attacks.
bility to membership inference attacks [53] on the WWT dataset.
As in [53], our metric is success rate, or the percentage of successful
trials in guessing whether a sample is in the training dataset. Naive
random guessing gives 50%, while we found an attack success rate
of 51%, suggesting robustness to membership inference in this case.
However, when we decrease the training set size, the attack success
the attack success rate is as high as 99.5%.Our results suggest a
practical guideline: to be more robust against membership attacks,
use more training data. This contradicts the common practice of
subsetting for better privacy [91].
Summary and Implications: The privacy properties of GANs
cate attribute distributions for masking business secrets, and there
appear to be simple techniques for preventing common membership
viding strong theoretical privacy guarantees. We find that existing
approaches for providing DP destroy fidelity beyond recognition,
and are not a viable solution.
More broadly, we believe there is value to exploring different
privacy metrics. DP is designed for datasets where one sample
corresponds to one user. However, many datasets of interest have
other attacks like model inversion [38, 57]. So it is not clear whether
DP is a good metric for this class of data.
CONCLUSIONS
vacy properties of GANs require further research for data holders
to confidently use such workflows. Moreover, many networking
datasets require significantly more complexity than DG is currently
able to handle, such as causal interactions between stateful agents.
which practitioners can model changes in the underlying system
larger changes may alter the physical system model such that the
model individual agent behavior. We hope that the initial promise
and open questions inspire further work from theoreticians and
practitioners to help break the impasse in data sharing.
does not raise any ethical issues as the public datasets do not contain
any personal information.
ACKNOWLEDGEMENTS
The authors would like to thank Shivani Shekhar for assistance
tin Otto, and Abishek Herle for valuable discussions. The authors
sightful suggestions. This work was supported in part by faculty
research awards from Google and JP Morgan Chase, as well as a
tional Science Foundation Convergence Accelerator award 2040675
and the U.S. Army Combat Capabilities Development Command
Army Research Laboratory under Cooperative Agreement Number
clusions contained in this document are those of the authors and
ther expressed or implied, of the Combat Capabilities Development
Command Army Research Laboratory or the U.S. Government. The
U.S. Government is authorized to reproduce and distribute reprints
for Government purposes notwithstanding any copyright notation
the Bridges system [87], which is supported by NSF award number
IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
REFERENCES
[3] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov,
ceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
Security. ACM, 308–318.
[4] Galen Andrew, Steve Chien, and Nicolas Papernot. 2019. TensorFlow Privacy.
[5] Spyros Antonatos, Kostas G Anagnostakis, and Evangelos P Markatos. 2004.
Generating realistic workloads for network intrusion detection systems. In
Proceedings of the 4th international workshop on Software and performance. 207–
[6] Martin Arjovsky, Soumith Chintala, and Léon Bottou. 2017. Wasserstein gan.
[7] Sanjeev Arora and Yi Zhang. 2017. Do gans actually learn the distribution? an
[8] Lars Backstrom, Cynthia Dwork, and Jon Kleinberg. 2007. Wherefore art thou
raphy. In Proceedings of the 16th international conference on World Wide Web.
ACM, 181–190.
tial privacy has disparate impact on model accuracy. In Advances in Neural
Information Processing Systems. 15453–15462.
[10] Jushan Bai and Shuzhong Shi. 2011. Estimating high dimensional covariance
ing the reliability of mobile broadband networks. In Proceedings of the 2014
Conference on Internet Measurement Conference. ACM, 45–58.
[12] Kevin Bauer, Damon McCoy, Ben Greenstein, Dirk Grunwald, and Douglas
national Symposium on Privacy Enhancing Technologies Symposium. Springer,
generative deep neural networks support clinical data sharing. Circulation:
fic characteristics of data centers in the wild. In Proceedings of the 10th ACM
SIGCOMM conference on Internet measurement. ACM, 267–280.
ing and Improving the Reliability of Broadband Internet Access. arXiv preprint
[17] Hans Buehler, Blanka Horvath, Terry Lyons, Imanol Perez Arribas, and Ben
[18] T Tony Cai, Zhao Ren, Harrison H Zhou, et al. 2016. Estimating structured
[19] Maria Carla Calzarossa, Luisa Massari, and Daniele Tessera. 2016. Workload
[20] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song.
2019. The secret sharer: Evaluating and testing unintended memorization in
provisioning cost in cloud computing. IEEE transactions on services Computing
taxonomy of membership inference attacks against gans.
arXiv preprint
[23] Li Chen, Justinas Lingys, Kai Chen, and Feng Liu. 2018. AuTO: scaling deep
Proceedings of the 2018 Conference of the ACM Special Interest Group on Data
Communication. ACM, 191–205.
[24] Edward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F Stewart,
[26] Hazy Company. 2020. Hazy builds on new technique to generate sequential
[27] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell
Sears. 2010. Benchmarking cloud serving systems with YCSB. In Proceedings of
the 1st ACM symposium on Cloud computing. 143–154.
[29] Yves Denneulin, Emmanuel Romagnoli, and Denis Trystram. 2004. A synthetic
workload generator for cluster computing. In 18th International Parallel and
Distributed Processing Symposium, 2004. Proceedings. IEEE, 243.
[30] Sheng Di and Franck Cappello. 2015. GloudSim: Google trace based cloud
1571–1590.
[31] Sheng Di, Derrick Kondo, and Franck Cappello. 2014. Characterizing and
Intelligence.
[33] Cynthia Dwork. 2008. Differential privacy: A survey of results. In International
Conference on Theory and Applications of Models of Computation. Springer, 1–19.
[34] Cristóbal Esteban, Stephanie L Hyland, and Gunnar Rätsch. [n. d.]. RCGAN
[36] William Fedus, Ian Goodfellow, and Andrew M Dai. 2018. MaskGAN: better
tion algorithms based on distributed learning automata and weighted association
[38] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and
study of personalized warfarin dosing. In 23rd {USENIX} Security Symposium
Greenspan. 2018. Synthetic data augmentation using GAN for improved liver
lesion classification. In 2018 IEEE 15th international symposium on biomedical
[40] Lorenzo Frigerio, Anderson Santana de Oliveira, Laurent Gomez, and Patrick
Duverger. 2019. Differentially Private Generative Adversarial Networks for Time
Series, Continuous, and Discrete Open Data. In IFIP International Conference on
ICT Systems Security and Privacy Protection. Springer, 151–164.
[41] Rao Fu, Jie Chen, Shutian Zeng, Yiping Zhuang, and Agus Sudjianto. 2019. Time
Series Simulation by Conditional Generative Adversarial Net. arXiv preprint
on Supercomputing. ACM, 41.
[45] Ian Goodfellow. 2016. NIPS 2016 tutorial: Generative adversarial networks.
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in neural information processing systems. 2672–2680.
[48] Robert Grandl, Ganesh Ananthanarayanan, Srikanth Kandula, Sriram Rao, and
[49] John T Guibas, Tejpal S Virdi, and Peter S Li. 2017. Synthetic medical images
from dual generative adversarial networks. arXiv preprint arXiv:1709.01872
[50] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and
Aaron C Courville. 2017. Improved training of wasserstein gans. In Advances in
Neural Information Processing Systems. 5767–5777.
[51] Alon Halevy, Peter Norvig, and Fernando Pereira. 2009. The unreasonable
Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
[52] Changhee Han, Hideaki Hayashi, Leonardo Rundo, Ryosuke Araki, Wataru
Shimoda, Shinichi Muramatsu, Yujiro Furukawa, Giancarlo Mauri, and Hideki
[53] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. 2019.
LOGAN: Membership Inference Attacks Against Generative Models. Proceedings
[54] Keqiang He, Alexis Fisher, Liang Wang, Aaron Gember, Aditya Akella, and
Thomas Ristenpart. 2013. Next stop, the cloud: Understanding modern web
service deployment in ec2 and azure. In Proceedings of the 2013 conference on
Internet measurement conference. ACM, 177–190.
[55] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and
to a local nash equilibrium. In Advances in neural information processing systems.
6626–6637.
[56] Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. 2012. Neural networks
ceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
Security. 603–618.
[60] Junchen Jiang, Rajdeep Das, Ganesh Ananthanarayanan, Philip A Chou, Venkata
Padmanabhan, Vyas Sekar, Esbjorn Dominique, Marcin Goliszewski, Dalibor
Kukoleca, Renat Vafin, et al. 2016. Via: Improving internet telephony call quality
using predictive relay selection. In Proceedings of the 2016 ACM SIGCOMM
Conference. ACM, 286–299.
[61] Junchen Jiang, Vyas Sekar, Henry Milner, Davis Shepherd, Ion Stoica, and Hui
Zhang. 2016. CFA: A Practical Prediction System for Video QoE Optimization..
In NSDI. 137–150.
[62] James Jordon, Daniel Jarrett, Jinsung Yoon, Paul Elbers, Patrick Thoral, Ari
Ercole, Cheng Zhang, Danielle Belgrave, and Mihaela van der Schaar. [n. d.].
[65] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive
growing of gans for improved quality, stability, and variation. arXiv preprint
[67] Tiancheng Li and Ninghui Li. 2009. On the tradeoff between privacy and
utility in data publishing. In Proceedings of the 15th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 517–526.
generation for network simulation. ACM Transactions on Modeling and Computer
[69] Zhijing Li, Zihui Ge, Ajay Mahimkar, Jia Wang, Ben Y Zhao, Haitao Zheng,
Joanne Emmons, and Laura Ogden. 2018.
Predictive Analysis in Network
Function Virtualization. In Proceedings of the Internet Measurement Conference
2018. ACM, 161–167.
[70] Zinan Lin, Ashish Khetan, Giulia Fanti, and Sewoong Oh. 2018. PacGAN: The
power of two samples in generative adversarial networks. In Advances in Neural
Information Processing Systems. 1505–1514.
[71] Ning Liu, Zhe Li, Jielong Xu, Zhiyuan Xu, Sheng Lin, Qinru Qiu, Jian Tang,
and Yanzhi Wang. 2017. A hierarchical framework of cloud resource allocation
and power management using deep reinforcement learning. In ICDCS. IEEE,
information processing systems. 700–709.
[73] Deborah Magalhães, Rodrigo N Calheiros, Rajkumar Buyya, and Danielo G
Gomes. 2015. Workload modeling for resource usage analysis and simulation in
[74] Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. 2016.
Resource management with deep reinforcement learning. In Proceedings of the
15th ACM Workshop on Hot Topics in Networks. ACM, 50–56.
8459–8463.
[77] Tony McGregor, Shane Alcock, and Daniel Karrenberg. 2010. The RIPE NCC
internet measurement data repository. In International Conference on Passive
and Active Network Measurement. Springer, 111–120.
ology. In Performance Evaluation of Computer and Communication Systems.
Springer, 359–393.
[79] Benjamin Melamed and Jon R Hill. 1993. Applications of the TES modeling
IEEE, 1330–1338.
ology: Modeling empirical stationary time series. In Proceedings of the 24th
conference on Winter simulation. ACM, 135–144.
[83] Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ousterhout. 2018.
Priorities. In SIGCOMM.
[84] Ismael Solis Moreno, Peter Garraghan, Paul Townend, and Jie Xu. 2014. Analysis,
large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on.
IEEE, 111–125.
tion based on web usage and domain knowledge. IEEE Transactions on Knowledge
[87] Nicholas A. Nystrom, Michael J. Levine, Ralph Z. Roskies, and J. Ray Scott. 2015.
Bridges: A Uniquely Flexible HPC Resource for New Communities and Data
Analytics. In Proceedings of the 2015 XSEDE Conference: Scientific Advancements
[88] Augustus Odena, Christopher Olah, and Jonathon Shlens. 2017. Conditional
[89] Paul Ohm. 2009. Broken promises of privacy: Responding to the surprising
release. In 2012 IEEE Network Operations and Management Symposium. IEEE,
1279–1286.
[92] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
and Xi Chen. 2016. Improved techniques for training gans. In Advances in Neural
Information Processing Systems. 2234–2242.
[94] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017.
Membership inference attacks against machine learning models. In 2017 IEEE
[95] Leszek Sliwko and Vladimir Getov. 2016. AGOCSâĂŤAccurate Google Cloud
Simulator Framework. In 2016 Intl IEEE Conferences on Ubiquitous Intelligence &
nications, Cloud and Big Data Computing, Internet of People, and Smart World
[96] Boogie Software. 2020. Synthesizing series of transactions with a Generative
tion. In Proceedings of the 4th ACM SIGCOMM conference on Internet measurement.
ACM, 68–81.
[98] Joel Sommers, Rhys Bowden, Brian Eriksson, Paul Barford, Matthew Roughan,
Proceedings IEEE INFOCOM. IEEE, 2363–2371.
[99] Joel Sommers, Vinod Yegneswaran, and Paul Barford. 2004. A framework
for malicious workload generation. In Proceedings of the 4th ACM SIGCOMM
conference on Internet measurement. 82–87.
[100] Charles Spearman. 1904. The proof and measurement of association between
IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
[101] Akash Srivastava, Lazar Valkov, Chris Russell, Michael U Gutmann, and Charles
Sutton. 2017. Veegan: Reducing mode collapse in gans using implicit variational
learning. In Advances in Neural Information Processing Systems. 3308–3318.
2000. Web usage mining: Discovery and applications of usage patterns from
[103] Mudhakar Srivatsa and Mike Hicks. 2012. Deanonymizing mobility traces: Using
Computer and communications security. 628–637.
surements. In Proceedings of the 2017 Internet Measurement Conference. ACM,
[105] Latanya Sweeney. 2000. Simple demographics often identify people uniquely.
[107] Vasily Tarasov, Erez Zadok, and Spencer Shepler. 2016. Filebench: A flexible
zlewood, S. Lathrop, D. Lifka, G. D. Peterson, R. Roskies, J. R. Scott, and N.
[109] Kashi Venkatesh Vishwanath and Amin Vahdat. 2009. Swing: Realistic and
and F Donelson Smith. 2006. Tmix: a tool for generating realistic TCP application
[111] Ronald J Williams and David Zipser. 1989. A learning algorithm for continually
tially private generative adversarial network. arXiv preprint arXiv:1802.06739
[113] Chugui Xu, Ju Ren, Deyu Zhang, Yaoxue Zhang, Zhan Qin, and Kui Ren. 2019.
GANobfuscator: Mitigating information leakage under GAN via differential
2358–2371.
[114] Qiantong Xu, Gao Huang, Yang Yuan, Chuan Guo, Yu Sun, Felix Wu, and Kilian
Weinberger. 2018. An empirical study on evaluation metrics of generative
[115] Jianwei Yin, Xingjian Lu, Xinkui Zhao, Hanwei Chen, and Xue Liu. 2014. BURSE:
[116] Jinsung
repository.
Generative Adversarial Networks. In Advances in Neural Information Processing
Systems. 5509–5519.
[118] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. SeqGAN: Sequence
Generative Adversarial Nets with Policy Gradient.. In AAAI. 2852–2858.
[119] Edvin Listo Zec, Henrik Arnelid, and Nasser Mohammadiha. 2019. Recurrent
Conditional GANs for Time Series Sensor Modelling. In Time Series Workshop
Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
straints, we did not use the entire dataset. Instead, we uniformly
surement records to form our dataset. This sample was collected
after filtering out the following categories:
of the end event does not match the ending timestamp of the
The maximum measurement length in this dataset is 2497, however,
97.06% samples have length within 50. The schema of this dataset
is in Table 6.
Wikipedia Web Traffic Dataset: The original datasets consists
of 145k samples. After removing samples with missing data, 117k
uation. All samples have measurement length 550. The schema of
this dataset is in Table 7.
FCC MBA dataset: We used the latest cleaned data published by
FCC MBA in December 2018 [25]. This datasets contains hourly
traffic measurements from 4378 homes in September and October
2017. However, a lot of measurements are missing in this dataset.
have complete network usage measurements every hour. This small
sample set will make us hard to understand the actual dynamic
patterns in this dataset. To increase number of valid samples, we
take the average of measurements every 6 hours for each home. As
long as there is at least one measurement in each 6 hours period,
we regard it as a valid sample. Using this way, we get 739 valid
which we sample 600 samples for our evaluation. All samples have
measurement length 56. The schema of this dataset is in Table 8.
IMPLEMENTATION DETAILS
hidden layers and 100 units in each layer. Measurement generator is
ical measurement and metadata output. Sigmoid or tanh is applied
for continuous measurement and metadata output, depending on
den layers and 200 units in each layer. Gradient penalty weight was
10.0 as suggested in [50]. The network was trained using Adam
optimizer with learning rate of 0.001 and batch size of 100 for both
generators and discriminators.
Loss function: As mentioned in §3.3.2, Wasserstein loss has been
widely adopted for improving training stability and alleviating
mode collapse. In our own empirical explorations, we find that
Description
Possible Values
end event type
The reason that the
task finishes
FAIL, KILL, EVICT,
Measurements
Description
Possible Values
Mean CPU rate
float numbers
Maximum CPU rate
float numbers
pled uniformly on
float numbers
Canonical memory
usage measurement
float numbers
assigned memory
Memory assigned
to the container
float numbers
cal memory usage
float numbers
Linux page cache
mapped into any
userspace process
float numbers
total page cache
Total Linux page
float numbers
local disk space
Runtime local disk
capacity usage
float numbers
Timestamp Discription
Possible Values
The timestamp that the measurement
was conducted on. Different task may
have different number of measurement
ments are described in more detail in [90].
gorical variables. Because categorical variables are prominent in
our domain, we use Wasserstein loss.
bine the loss functions of the two discriminators by a weighting
parameter α. More specifically, the loss function is
on metadata. As with all GANs, the generator and discriminators
tectures, we did not observe problems with training instability, and
on our datasets, convergence required only up to 200,000 batches
IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
Description
Possible Values
main name of the
Wikipedia page
zh.wikipedia.org,
mons.wikimedia.org,
access type
The access method
The agent type
Measurements
Description
Possible Values
Timestamp Discription
Possible Values
The date that the page view is counted
Table 7: Schema of WWT dataset
Description
Possible Values
technology
connection
technology of the
cable, fiber, etc.
provider of the unit
AT&T, Verizon, etc.
The state where the
unit is located
PA, CA, etc.
Measurements
Description
Possible Values
ping loss rate
UDP ping loss rate
to the server that
has lowest loss rate
within the hour
float numbers
Timestamp Discription
Possible Values
The time of the measurement hour
Table 8: Schema of MBA dataset
Generation flag for variable length: Time series may have different
lengths. For example, in GCUT dataset, different jobs have different
length. However, that would introduce a confusion on whether a
nal measurements, we add generation flag to each time step: [1, 0]
measurements
Generation
if the time series does not end at this time step, and [0, 1] if the
outputs generation flag [p1,p2] through a softmax output layer,
mine whether we should continue unrolling the RNN to the next
time step. One way to interpret this is that p1 gives the probability
that the time series should continue at this time step. Therefore, if
p1 < p2, we stop generation and pad all future measurements with
fed to the discriminator as part of the features, so the generator can
also learn sample length characteristics.
We also want to highlight that if the user wants to control the
length of the generated samples, our architecture can also support
this by iterating the RNN generator for the given desired number
the next. The AR model was an MLP with 4 hidden layers and 200
units in each layer. The MLP was trained using Adam optimizer
[66] with learning rate of 0.001 and batch size of 100.
[58] variant of RNN. It is 1 layers of LSTM with 100 units. The
network was trained using Adam optimizer with learning rate of
0.001 and batch size of 100.
Naive GAN: The generator and discriminator are MLPs with 4
hidden layers and 200 units in each layer. Gradient penalty weight
was 10.0 as suggested in [50]. The network was trained using Adam
optimizer with learning rate of 0.001 and batch size of 100 for both
generator and discriminator.
ADDITIONAL FIDELITY RESULTS
Temporal length: Figure 18 shows the length distribution of
DG and baselines in GCUT dataset. It is clear that DG has the best
Metadata distribution: Figure 19 shows the histogram of Wikipedia
domain of Naive GAN and DG. DG learns the distribution fairly
well, whereas naive GAN cannot.
total bandwidth for DSL and cable users in MBA dataset. Figures
the fact that cable users consume more bandwidth than DSL users.
However, DG appears to excel in regions of the distribution with less
data, e.g., very small bandwidth levels. In both cases, DG captures
the bandwidth distribution better than the other baselines. This
Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions
IMC ’20, October 27–29, 2020, Virtual Event, USA
DoppelGANger
Figure 18: Histogram of task duration for the GCUT dataset.
DoppelGANger gives the best fidelity.
commons.wikimedia.org
de.wikipedia.org
en.wikipedia.org
es.wikipedia.org
fr.wikipedia.org
ja.wikipedia.org
ru.wikipedia.org
www.mediawiki.org
zh.wikipedia.org
DoppelGANger
commons.wikimedia.org
de.wikipedia.org
en.wikipedia.org
es.wikipedia.org
fr.wikipedia.org
ja.wikipedia.org
ru.wikipedia.org
www.mediawiki.org
zh.wikipedia.org
Wikipedia domain
Histograms
WWT dataset.
means that DG has a high fidelity on measurement distribution,
influence the measurements.
DoppelGANger
DoppelGANger
Figure 20: Total bandwidth usage in 2 weeks in MBA dataset
Generated sample 1st nearest
2nd nearest
3rd nearest
Figure 21: Three time series samples selected uniformly
at random from the synthetic dataset generated using
Generated sample 1st nearest
2nd nearest
3rd nearest
Figure 22: Three time series samples selected uniformly
at random from the synthetic dataset generated using
DG does not simply memorize training samples: Figure 21,
22, 23 show the some generated samples from DG and their nearest
datasets. The results show that DG is not memorizing training
samples. To achieve the good fidelity results we have shown before,
DG must indeed learn the underlying structure of the samples.
IMC ’20, October 27–29, 2020, Virtual Event, USA
Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar
Generated sample 1st nearest
2nd nearest
3rd nearest
Figure 23: Three time series samples selected uniformly
at random from the synthetic dataset generated using
KernelRidge
LinearRegression
DoppelGANger
Figure 24: Coefficient of determination for WWT time series
forecasting. Higher is better.
ADDITIONAL CASE STUDY RESULTS
eling task involves forecasting of the page views for next 50 days,
series as input and predict the next 50 time steps. For this purpose,
we train various regression models: an MLP with five hidden layers
a linear regression model, and a Kernel regression model using an
ficient of determination, R2, which captures how well a regression
model describes a particular dataset.11
Figure 24 shows the R2 for each of these models for each of our
generative models and the real data. Here we train each regression
it is to be expected that real data performs best. It is clear that
DG performs better than other baselines for all regression models.
Note that sometimes RNN, AR, and naive GANs baselines have
large negative R2 which are therefore not visualized in this plot.
tion algorithms on DG’s and baselines’ generated data. Combined
with 5, we see that DG and AR are the best for preserving ranking
of prediction algorithms.
i yi is the mean
DoppelGANgerRNN
NaiveBayes
LogisticRegr.
DecisionTree
Figure 25: Ranking of end event type prediction algorithms
on GCUT dataset.
DoppelGANgerRNN
LinearRegr.
KernelRidge
Figure 26: Ranking of traffic prediction algorithms on WWT
