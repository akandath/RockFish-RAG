{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35904774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chromadb langchain sentence-transformers\n",
    "import os\n",
    "os.environ[\"CHROMA_TELEMETRY\"] = \"0\"\n",
    "\n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e5d6c8",
   "metadata": {},
   "source": [
    "# Data Ingestion and Storage\n",
    "\n",
    "This notebook implements the core data ingestion and chunking pipeline for building a Retrieval-Augmented Generation (RAG) system on the Rockfish platform. Key steps include:\n",
    "\n",
    "**Loading documents** (e.g., from text files)  \n",
    "**Chunking** them into smaller passages for better searchability  \n",
    "**Generating embeddings** using `sentence-transformers/all-MiniLM-L6-v2`  \n",
    "**Storing** these chunks in a Chroma vector database (or other vector DB)  \n",
    "**Validating** that embeddings and chunks were successfully stored\n",
    "\n",
    "This sets up the **retrieval layer** of your RAG pipelineâ€”ready for LLM integration later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90dc2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 88 documents\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "DATA_FOLDER = 'Data'\n",
    "docs = []\n",
    "for file_name in os.listdir(DATA_FOLDER):\n",
    "    if file_name.endswith('.txt'):\n",
    "        loader = TextLoader(os.path.join(DATA_FOLDER, file_name))\n",
    "        loaded_docs = loader.load()\n",
    "        docs.extend(loaded_docs)\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f5752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1612 chunks\n"
     ]
    }
   ],
   "source": [
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"Generated {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2543784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(99373) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks stored in Chroma DB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rf_rag/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Chroma vectorstore\n",
    "persist_directory = \"chroma_db\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "chroma_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "chroma_db.persist()\n",
    "print(\"Chunks stored in Chroma DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7913949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rf_rag/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Chunks:\n",
      "\n",
      " Chunk 1:\n",
      "(Optional) Rockfish Integration\n",
      "\n",
      "Follow this tutorial to understand how to integrate the Rockfish platform into your pipeline.\n",
      "\n",
      "Load a dataset\n",
      "\n",
      " Chunk 2:\n",
      "Now that you have followed our tutorial to understand the basics of how to integrate Rockfish in your Ops pipeline, lets focus on your use cases for Synthetic data.\n",
      "\n",
      " Chunk 3:\n",
      "Now that you have followed our tutorial to understand the basics of how to integrate Rockfish in your Ops pipeline, lets focus on your use cases for Synthetic data.\n",
      "Follow these use case tutorials to familiarize yourself with how to use Rockfish platform to solve your use case.\n",
      "These examples provide a starting point for common use cases which you can modify to suit your specific needs.\n",
      "\n",
      " Chunk 4:\n",
      "Now that you have followed our tutorial to understand the basics of how to integrate Rockfish in your Ops pipeline, lets focus on your use cases for Synthetic data.\n",
      "Follow these use case tutorials to familiarize yourself with how to use Rockfish platform to solve your use case.\n",
      "These examples provide a starting point for common use cases which you can modify to suit your specific needs.\n",
      "\n",
      " Chunk 5:\n",
      "a One-Stop synthetic data workbench that\n",
      "automatically customizes itself to your\n",
      "data\n",
      "Rockfish plugs directly into your\n",
      "existing data ingestion pipelines\n",
      "automatically chooses the best\n",
      "representation for your data and trains\n"
     ]
    }
   ],
   "source": [
    "# Query / retrieval function\n",
    "def retrieve_top_k_chunks(question, top_k=5):\n",
    "    retriever = chroma_db.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    relevant_docs = retriever.get_relevant_documents(question)\n",
    "    return relevant_docs\n",
    "\n",
    "# Example query\n",
    "question = \"How does Rockfish integrate with existing data pipelines?\"\n",
    "top_chunks = retrieve_top_k_chunks(question, top_k=5)\n",
    "print(\"\\nTop Chunks:\")\n",
    "for idx, doc in enumerate(top_chunks, start=1):\n",
    "    print(f\"\\n Chunk {idx}:\\n{doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rf_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
