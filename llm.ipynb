{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347812a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708ed48a",
   "metadata": {},
   "source": [
    "# LLM Layer (RAG Chatbot)\n",
    "\n",
    "This notebook sets up the final **LLM layer** for our Retrieval-Augmented Generation (RAG) chatbot. \n",
    "\n",
    "✅ It connects to a local Chroma DB containing vector embeddings of the documents.  \n",
    "✅ Uses a lightweight open-source model (**Flan-T5**) via HuggingFace Pipeline for natural language generation.  \n",
    "✅ Combines both for a **conversational chatbot** that:\n",
    "- Retrieves relevant document chunks from Chroma DB.\n",
    "- Generates contextual answers with LLM.\n",
    "- Maintains conversational memory for a natural chat experience.\n",
    "\n",
    "This is the last step of our RAG pipeline, enabling us to answer user questions interactively and conversationally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d6f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma retriever\n",
    "persist_directory = \"chroma_db\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "retriever = chroma_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffd953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq API function\n",
    "import requests\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "def get_groq_completion(prompt: str) -> str:\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"deepseek-r1-distill-llama-70b\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a helpful assistant. Provide clear, concise, and professional answers. The other thing to consider is potential buyers might interact with you so you should subtly sell the product without being too pushy. \"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(\n",
    "        \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "        headers=headers,\n",
    "        json=data\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    result = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "    if \"<think>\" in result and \"</think>\" in result:\n",
    "        result = result.split(\"</think>\")[-1].strip()\n",
    "\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ad043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final context-aware QA function\n",
    "def answer_with_context(question: str, top_k: int = 10):\n",
    "    top_chunks: list[Document] = retriever.get_relevant_documents(question, top_k=top_k)\n",
    "    combined_context = \"\\n\\n\".join(chunk.page_content for chunk in top_chunks)\n",
    "\n",
    "    prompt = (\n",
    "        f\"Context:\\n{combined_context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Answer concisely and professionally.\"\n",
    "    )\n",
    "    answer = get_groq_completion(prompt)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6db42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I integrate the Rockfish API in my existing stack? give me clear and concise code examples. Imagine that I have the API key\"\n",
    "answer = answer_with_context(question)\n",
    "print(\"\\nAnswer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c06002",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What types of data can it create? Can it create data for a specific industry?\"\n",
    "answer = answer_with_context(question)\n",
    "print(\"\\nAnswer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363fb20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is the research behind the product? What is the technology behind the product? who are the main researchers behind the product?\"\n",
    "answer = answer_with_context(question)\n",
    "print(\"\\nAnswer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4208da",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"can you cite some technicalities of the product from a research standpoint? What are the main technical features of the product?\"\n",
    "answer = answer_with_context(question)\n",
    "print(\"\\nAnswer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what do you customers say about the product? Give concrete examples and cite the sources. How do I know whether the product creates useful data?\"\n",
    "answer = answer_with_context(question)\n",
    "print(\"\\nAnswer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I am on the fence about buying the product. Can you convince me to buy it? Why rockfish?\"\n",
    "answer = answer_with_context(question)\n",
    "print(\"\\nAnswer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edeece",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"tell me about your latest presence in the news. What are the latest developments in the company? Give concrete events and cite the sources.\"\n",
    "answer = answer_with_context(question)\n",
    "print(\"\\nAnswer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cc479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rf_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
